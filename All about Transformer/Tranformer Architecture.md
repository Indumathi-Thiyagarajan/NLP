Tranformer geneates text one word at a time. 
Up untill transformer comes, the next word prediction wasnt that great. 
Because neural network can shorter memory and couldnt remember longer context and more words. 
Then came the attention mechanism which gave more context with more words window.

![image](https://github.com/user-attachments/assets/50a67cc1-bb53-4d8e-8242-ecdf910e9545)


## Tokenizer:

    Tokenizer takes the input text and breaks them into individual pieces. These are words or either parts of words. 
    There are several trained tokenizer where I have tried some here. Different trained tokenizer behave differently, So we have to chose the tokenizer based on our usecase.
  Tokenization.ipynb    
